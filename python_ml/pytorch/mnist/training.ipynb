{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dac17fc",
   "metadata": {},
   "source": [
    "Imports and Seedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50538fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d77ded",
   "metadata": {},
   "source": [
    "Dataset Transforms (Equivalent to Transform enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0853fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transform_list(transform_names):\n",
    "    ops = []\n",
    "\n",
    "    for t in transform_names:\n",
    "        if t == \"Translate\":\n",
    "            ops.append(transforms.RandomAffine(0, translate=(0.1, 0.1)))\n",
    "        elif t == \"Shear\":\n",
    "            ops.append(transforms.RandomAffine(0, shear=10))\n",
    "        elif t == \"Scale\":\n",
    "            ops.append(transforms.RandomAffine(0, scale=(0.9, 1.1)))\n",
    "        elif t == \"Rotation\":\n",
    "            ops.append(transforms.RandomRotation(10))\n",
    "\n",
    "    return transforms.Compose(ops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff75b99",
   "metadata": {},
   "source": [
    "DatasetIdent Equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f11bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIdent:\n",
    "    PLAIN = \"Plain\"\n",
    "    ALL = \"All\"\n",
    "\n",
    "    def __init__(self, transforms=None):\n",
    "        self.transforms = transforms or []\n",
    "\n",
    "    def prepare(self, base_dataset):\n",
    "        if not self.transforms:\n",
    "            return base_dataset\n",
    "\n",
    "        tfm = build_transform_list(self.transforms)\n",
    "        return datasets.MNIST(\n",
    "            root=base_dataset.root,\n",
    "            train=base_dataset.train,\n",
    "            download=False,\n",
    "            transform=tfm\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        if not self.transforms:\n",
    "            return \"Plain\"\n",
    "        return \" \".join(self.transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1a063",
   "metadata": {},
   "source": [
    "Generate Dataset Idents (Exact Logic Port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380aa231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_idents(num_samples_base=None):\n",
    "    idents = []\n",
    "\n",
    "    transforms = [\"Shear\", \"Scale\", \"Rotation\", \"Translate\"]\n",
    "\n",
    "    for i in range(2 ** 4):\n",
    "        current = [transforms[j] for j in range(4) if (i >> j) & 1]\n",
    "\n",
    "        if len(current) == 4:\n",
    "            ident = DatasetIdent(transforms)\n",
    "        elif not current:\n",
    "            ident = DatasetIdent()\n",
    "        else:\n",
    "            ident = DatasetIdent(current)\n",
    "\n",
    "        size = num_samples_base * len(current) if num_samples_base else None\n",
    "        idents.append((ident, size))\n",
    "\n",
    "    return idents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103079f0",
   "metadata": {},
   "source": [
    "Build Training & Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e119107",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./data\"\n",
    "\n",
    "base_train = datasets.MNIST(root, train=True, download=True, transform=transforms.ToTensor())\n",
    "train_plain = Subset(base_train, range(0, 55_000))\n",
    "valid_plain = Subset(base_train, range(55_000, 60_000))\n",
    "\n",
    "train_idents = generate_idents(10_000)\n",
    "valid_idents = generate_idents(None)\n",
    "\n",
    "def compose_dataset(idents, base_subset):\n",
    "    datasets_out = []\n",
    "\n",
    "    for ident, size in idents:\n",
    "        ds = ident.prepare(base_subset.dataset)\n",
    "        ds = Subset(ds, base_subset.indices)\n",
    "\n",
    "        if size:\n",
    "            sampler = WeightedRandomSampler(\n",
    "                weights=[1.0] * len(ds),\n",
    "                num_samples=size,\n",
    "                replacement=True\n",
    "            )\n",
    "            datasets_out.append(DataLoader(ds, sampler=sampler))\n",
    "        else:\n",
    "            datasets_out.append(ds)\n",
    "\n",
    "    return ConcatDataset(datasets_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7bbfa",
   "metadata": {},
   "source": [
    "DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_dataset = compose_dataset(train_idents, train_plain)\n",
    "valid_dataset = compose_dataset(valid_idents, valid_plain)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928161f",
   "metadata": {},
   "source": [
    "Optimizer & LR Scheduler (Exact Burn Match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1.0,\n",
    "    weight_decay=5e-5\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[\n",
    "        optim.lr_scheduler.LinearLR(optimizer, start_factor=1e-8, end_factor=1.0, total_iters=2000),\n",
    "        optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2000),\n",
    "        optim.lr_scheduler.LinearLR(optimizer, start_factor=1e-2, end_factor=1e-6, total_iters=10000),\n",
    "    ],\n",
    "    milestones=[2000, 4000]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4c600",
   "metadata": {},
   "source": [
    "Training Loop with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x.squeeze(1))\n",
    "        loss = nn.CrossEntropyLoss()(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            out = model(x.squeeze(1))\n",
    "            loss = nn.CrossEntropyLoss()(out, y)\n",
    "            val_loss += loss.item()\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    val_loss /= len(valid_loader)\n",
    "    acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch}: val_loss={val_loss:.4f}, acc={acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb132b",
   "metadata": {},
   "source": [
    "Test Evaluation (Per DatasetIdent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a28d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_base = datasets.MNIST(root, train=False, download=True, transform=transforms.ToTensor())\n",
    "test_idents = generate_idents(None)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "for ident, _ in test_idents:\n",
    "    ds = ident.prepare(test_base)\n",
    "    loader = DataLoader(ds, batch_size=batch_size)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            out = model(x.squeeze(1))\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    print(f\"{ident}: accuracy = {correct / total:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
